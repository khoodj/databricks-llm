# #
# # This file is autogenerated by pip-compile with Python 3.11
# # by the following command:
# #
# #    pip-compile --output-file=requirements.txt requirements.in
# #
# aenum==3.1.15
#     # via gradientai
# aiohappyeyeballs==2.4.0
#     # via aiohttp
# aiohttp==3.10.5
#     # via
#     #   huggingface-hub
#     #   langchain
#     #   langchain-community
#     #   llama-index-core
#     #   llama-index-legacy
# aiosignal==1.3.1
#     # via aiohttp
# altair==5.4.1
#     # via streamlit
# anyio==4.4.0
#     # via
#     #   httpx
#     #   langfuse
#     #   openai
# asttokens==2.4.1
#     # via stack-data
# attrs==24.2.0
#     # via
#     #   aiohttp
#     #   jsonschema
#     #   referencing
# backoff==2.2.1
#     # via langfuse
# beautifulsoup4==4.12.3
#     # via llama-index-readers-file
# beyondllm==0.2.3
#     # via -r requirements.in
# blinker==1.8.2
#     # via streamlit
# boto3==1.35.10
#     # via cohere
# botocore==1.35.10
#     # via
#     #   boto3
#     #   s3transfer
# cachetools==5.5.0
#     # via
#     #   google-auth
#     #   streamlit
# certifi==2024.8.30
#     # via
#     #   httpcore
#     #   httpx
#     #   requests
# charset-normalizer==3.3.2
#     # via requests
# click==8.1.7
#     # via
#     #   nltk
#     #   streamlit
# cohere==5.9.0
#     # via llama-index-postprocessor-cohere-rerank
# colorama==0.4.6
#     # via
#     #   click
#     #   ipython
#     #   loguru
#     #   tqdm
# coloredlogs==15.0.1
#     # via onnxruntime
# comm==0.2.2
#     # via ipykernel
# contourpy==1.3.0
#     # via matplotlib
# cycler==0.12.1
#     # via matplotlib
# dataclasses-json==0.6.7
#     # via
#     #   langchain-community
#     #   llama-index-core
#     #   llama-index-legacy
# debugpy==1.8.5
#     # via ipykernel
# decorator==5.1.1
#     # via ipython
# deprecated==1.2.14
#     # via
#     #   llama-index-core
#     #   llama-index-legacy
# dirtyjson==1.0.8
#     # via
#     #   llama-index-core
#     #   llama-index-legacy
# distro==1.9.0
#     # via openai
# executing==2.1.0
#     # via stack-data
# faiss-cpu==1.9.0
#     # via -r requirements.in
# fastavro==1.9.5
#     # via cohere
# fastembed==0.3.6
#     # via llama-index-embeddings-fastembed
# filelock==3.15.4
#     # via
#     #   huggingface-hub
#     #   torch
#     #   transformers
# flatbuffers==24.3.25
#     # via onnxruntime
# fonttools==4.53.1
#     # via matplotlib
# frozenlist==1.4.1
#     # via
#     #   aiohttp
#     #   aiosignal
# fsspec==2024.6.1
#     # via
#     #   huggingface-hub
#     #   llama-index-core
#     #   llama-index-legacy
#     #   torch
# gitdb==4.0.11
#     # via gitpython
# gitpython==3.1.43
#     # via streamlit
# google-ai-generativelanguage==0.4.0
#     # via google-generativeai
# google-api-core[grpc]==2.19.2
#     # via
#     #   google-ai-generativelanguage
#     #   google-generativeai
# google-auth==2.34.0
#     # via
#     #   google-api-core
#     #   google-generativeai
# google-generativeai==0.4.1
#     # via llama-index-embeddings-gemini
# googleapis-common-protos==1.65.0
#     # via
#     #   google-api-core
#     #   grpcio-status
# gradientai==1.13.1
#     # via llama-index-llms-gradient
# greenlet==3.0.3
#     # via sqlalchemy
# grpcio==1.66.1
#     # via
#     #   google-api-core
#     #   grpcio-status
# grpcio-status==1.62.3
#     # via google-api-core
# h11==0.14.0
#     # via httpcore
# httpcore==1.0.5
#     # via httpx
# httpx==0.27.2
#     # via
#     #   cohere
#     #   langfuse
#     #   langsmith
#     #   llama-index-core
#     #   llama-index-legacy
#     #   llamaindex-py-client
#     #   openai
# httpx-sse==0.4.0
#     # via cohere
# huggingface-hub[inference]==0.24.6
#     # via
#     #   fastembed
#     #   llama-index-embeddings-huggingface
#     #   sentence-transformers
#     #   tokenizers
#     #   transformers
# humanfriendly==10.0
#     # via coloredlogs
# idna==3.8
#     # via
#     #   anyio
#     #   httpx
#     #   langfuse
#     #   requests
#     #   yarl
# ipykernel==6.29.5
#     # via -r requirements.in
# ipython==8.27.0
#     # via ipykernel
# jedi==0.19.1
#     # via ipython
# jinja2==3.1.4
#     # via
#     #   altair
#     #   pydeck
#     #   torch
# jmespath==1.0.1
#     # via
#     #   boto3
#     #   botocore
# joblib==1.4.2
#     # via
#     #   nltk
#     #   scikit-learn
# jsonpatch==1.33
#     # via langchain-core
# jsonpointer==3.0.0
#     # via jsonpatch
# jsonschema==4.23.0
#     # via altair
# jsonschema-specifications==2023.12.1
#     # via jsonschema
# jupyter-client==8.6.2
#     # via ipykernel
# jupyter-core==5.7.2
#     # via
#     #   ipykernel
#     #   jupyter-client
# kiwisolver==1.4.7
#     # via matplotlib
# langchain==0.2.16
#     # via
#     #   -r requirements.in
#     #   langchain-community
# langchain-community==0.2.16
#     # via
#     #   -r requirements.in
#     #   langchain-experimental
# langchain-core==0.2.38
#     # via
#     #   langchain
#     #   langchain-community
#     #   langchain-experimental
#     #   langchain-postgres
#     #   langchain-text-splitters
#     #   langgraph
#     #   langgraph-checkpoint
# langchain-experimental==0.0.65
#     # via -r requirements.in
# langchain-postgres==0.0.12
#     # via -r requirements.in
# langchain-text-splitters==0.2.4
#     # via langchain
# langfuse==2.52.1
#     # via -r requirements.in
# langgraph==0.2.19
#     # via -r requirements.in
# langgraph-checkpoint==1.0.12
#     # via langgraph
# langsmith==0.1.111
#     # via
#     #   -r requirements.in
#     #   langchain
#     #   langchain-community
#     #   langchain-core
# llama-index==0.10.27
#     # via beyondllm
# llama-index-agent-openai==0.2.9
#     # via
#     #   llama-index
#     #   llama-index-program-openai
# llama-index-cli==0.1.13
#     # via llama-index
# llama-index-core==0.10.68.post1
#     # via
#     #   llama-index
#     #   llama-index-agent-openai
#     #   llama-index-cli
#     #   llama-index-embeddings-adapter
#     #   llama-index-embeddings-fastembed
#     #   llama-index-embeddings-gemini
#     #   llama-index-embeddings-huggingface
#     #   llama-index-embeddings-openai
#     #   llama-index-finetuning
#     #   llama-index-indices-managed-llama-cloud
#     #   llama-index-llms-gradient
#     #   llama-index-llms-openai
#     #   llama-index-multi-modal-llms-openai
#     #   llama-index-postprocessor-cohere-rerank
#     #   llama-index-program-openai
#     #   llama-index-question-gen-openai
#     #   llama-index-readers-file
#     #   llama-index-readers-llama-parse
#     #   llama-index-readers-youtube-transcript
#     #   llama-parse
# llama-index-embeddings-adapter==0.1.3
#     # via llama-index-finetuning
# llama-index-embeddings-fastembed==0.1.7
#     # via -r requirements.in
# llama-index-embeddings-gemini==0.1.6
#     # via beyondllm
# llama-index-embeddings-huggingface==0.2.0
#     # via -r requirements.in
# llama-index-embeddings-openai==0.1.11
#     # via
#     #   llama-index
#     #   llama-index-cli
# llama-index-finetuning==0.1.5
#     # via -r requirements.in
# llama-index-indices-managed-llama-cloud==0.1.6
#     # via llama-index
# llama-index-legacy==0.9.48.post3
#     # via llama-index
# llama-index-llms-gradient==0.1.2
#     # via llama-index-finetuning
# llama-index-llms-openai==0.1.27
#     # via
#     #   llama-index
#     #   llama-index-agent-openai
#     #   llama-index-cli
#     #   llama-index-finetuning
#     #   llama-index-multi-modal-llms-openai
#     #   llama-index-program-openai
#     #   llama-index-question-gen-openai
# llama-index-multi-modal-llms-openai==0.1.9
#     # via llama-index
# llama-index-postprocessor-cohere-rerank==0.1.7
#     # via llama-index-finetuning
# llama-index-program-openai==0.1.7
#     # via
#     #   llama-index
#     #   llama-index-question-gen-openai
# llama-index-question-gen-openai==0.1.3
#     # via llama-index
# llama-index-readers-file==0.1.33
#     # via llama-index
# llama-index-readers-llama-parse==0.1.6
#     # via llama-index
# llama-index-readers-youtube-transcript==0.1.4
#     # via -r requirements.in
# llama-parse==0.4.9
#     # via llama-index-readers-llama-parse
# llamaindex-py-client==0.1.19
#     # via llama-index-indices-managed-llama-cloud
# loguru==0.7.2
#     # via fastembed
# markdown-it-py==3.0.0
#     # via rich
# markupsafe==2.1.5
#     # via jinja2
# marshmallow==3.22.0
#     # via dataclasses-json
# matplotlib==3.9.2
#     # via -r requirements.in
# matplotlib-inline==0.1.7
#     # via
#     #   ipykernel
#     #   ipython
# mdurl==0.1.2
#     # via markdown-it-py
# minijinja==2.2.0
#     # via huggingface-hub
# mmh3==4.1.0
#     # via fastembed
# mpmath==1.3.0
#     # via sympy
# msgpack==1.1.0
#     # via langgraph-checkpoint
# multidict==6.0.5
#     # via
#     #   aiohttp
#     #   yarl
# mypy-extensions==1.0.0
#     # via typing-inspect
# narwhals==1.6.0
#     # via altair
# nest-asyncio==1.6.0
#     # via
#     #   ipykernel
#     #   llama-index-core
#     #   llama-index-legacy
# networkx==3.3
#     # via
#     #   llama-index-core
#     #   llama-index-legacy
#     #   torch
# nltk==3.8.1
#     # via
#     #   beyondllm
#     #   llama-index-core
#     #   llama-index-legacy
# numpy==1.26.4
#     # via
#     #   beyondllm
#     #   contourpy
#     #   faiss-cpu
#     #   fastembed
#     #   langchain
#     #   langchain-community
#     #   langchain-postgres
#     #   llama-index-core
#     #   llama-index-legacy
#     #   matplotlib
#     #   onnx
#     #   onnxruntime
#     #   pandas
#     #   pgvector
#     #   pyarrow
#     #   pydeck
#     #   scikit-learn
#     #   scipy
#     #   sentence-transformers
#     #   streamlit
#     #   transformers
# onnx==1.16.2
#     # via fastembed
# onnxruntime==1.19.0
#     # via fastembed
# openai==1.20.0
#     # via
#     #   beyondllm
#     #   llama-index-agent-openai
#     #   llama-index-legacy
# orjson==3.10.7
#     # via langsmith
# packaging==24.1
#     # via
#     #   altair
#     #   faiss-cpu
#     #   huggingface-hub
#     #   ipykernel
#     #   langchain-core
#     #   langfuse
#     #   marshmallow
#     #   matplotlib
#     #   onnxruntime
#     #   streamlit
#     #   transformers
# pandas==2.0.3
#     # via
#     #   beyondllm
#     #   llama-index-core
#     #   llama-index-legacy
#     #   streamlit
# parameterized==0.9.0
#     # via cohere
# parso==0.8.4
#     # via jedi
# pgvector==0.2.5
#     # via langchain-postgres
# pillow==10.4.0
#     # via
#     #   fastembed
#     #   llama-index-core
#     #   matplotlib
#     #   sentence-transformers
#     #   streamlit
# platformdirs==4.2.2
#     # via jupyter-core
# prompt-toolkit==3.0.47
#     # via ipython
# proto-plus==1.24.0
#     # via
#     #   google-ai-generativelanguage
#     #   google-api-core
# protobuf==4.25.4
#     # via
#     #   google-ai-generativelanguage
#     #   google-api-core
#     #   google-generativeai
#     #   googleapis-common-protos
#     #   grpcio-status
#     #   onnx
#     #   onnxruntime
#     #   proto-plus
#     #   streamlit
# psutil==6.0.0
#     # via ipykernel
# psycopg==3.2.3
#     # via langchain-postgres
# psycopg-pool==3.2.3
#     # via langchain-postgres
# pure-eval==0.2.3
#     # via stack-data
# pyarrow==17.0.0
#     # via streamlit
# pyasn1==0.6.0
#     # via
#     #   pyasn1-modules
#     #   rsa
# pyasn1-modules==0.4.0
#     # via google-auth
# pydantic==1.10.18
#     # via
#     #   beyondllm
#     #   cohere
#     #   google-generativeai
#     #   gradientai
#     #   langchain
#     #   langchain-core
#     #   langfuse
#     #   langsmith
#     #   llama-index-core
#     #   llamaindex-py-client
#     #   openai
# pydantic-core==2.23.1
#     # via cohere
# pydeck==0.9.1
#     # via streamlit
# pygments==2.18.0
#     # via
#     #   ipython
#     #   rich
# pyparsing==3.1.4
#     # via matplotlib
# pypdf==4.2.0
#     # via
#     #   beyondllm
#     #   llama-index-readers-file
# pyreadline3==3.4.1
#     # via humanfriendly
# pysbd==0.3.4
#     # via beyondllm
# pystemmer==2.2.0.1
#     # via fastembed
# python-dateutil==2.9.0.post0
#     # via
#     #   botocore
#     #   gradientai
#     #   jupyter-client
#     #   matplotlib
#     #   pandas
# python-dotenv==1.0.1
#     # via -r requirements.in
# pytz==2024.1
#     # via pandas
# pywin32==306
#     # via jupyter-core
# pyyaml==6.0.1
#     # via
#     #   beyondllm
#     #   huggingface-hub
#     #   langchain
#     #   langchain-community
#     #   langchain-core
#     #   llama-index-core
#     #   transformers
# pyzmq==26.2.0
#     # via
#     #   ipykernel
#     #   jupyter-client
# referencing==0.35.1
#     # via
#     #   jsonschema
#     #   jsonschema-specifications
# regex==2024.4.16
#     # via
#     #   beyondllm
#     #   nltk
#     #   tiktoken
#     #   transformers
# requests==2.32.3
#     # via
#     #   cohere
#     #   fastembed
#     #   google-api-core
#     #   huggingface-hub
#     #   langchain
#     #   langchain-community
#     #   langsmith
#     #   llama-index-core
#     #   llama-index-legacy
#     #   streamlit
#     #   tiktoken
#     #   transformers
#     #   youtube-transcript-api
# rich==13.8.0
#     # via streamlit
# rpds-py==0.20.0
#     # via
#     #   jsonschema
#     #   referencing
# rsa==4.9
#     # via google-auth
# s3transfer==0.10.2
#     # via boto3
# safetensors==0.4.4
#     # via transformers
# scikit-learn==1.5.1
#     # via sentence-transformers
# scipy==1.14.1
#     # via
#     #   scikit-learn
#     #   sentence-transformers
# sentence-transformers==2.7.0
#     # via
#     #   llama-index-embeddings-huggingface
#     #   llama-index-finetuning
# six==1.16.0
#     # via
#     #   asttokens
#     #   python-dateutil
# smmap==5.0.1
#     # via gitdb
# sniffio==1.3.1
#     # via
#     #   anyio
#     #   httpx
#     #   openai
# snowballstemmer==2.2.0
#     # via fastembed
# soupsieve==2.6
#     # via beautifulsoup4
# sqlalchemy[asyncio]==2.0.29
#     # via
#     #   beyondllm
#     #   langchain
#     #   langchain-community
#     #   langchain-postgres
#     #   llama-index-core
#     #   llama-index-legacy
# stack-data==0.6.3
#     # via ipython
# streamlit==1.38.0
#     # via -r requirements.in
# striprtf==0.0.26
#     # via llama-index-readers-file
# sympy==1.13.2
#     # via
#     #   onnxruntime
#     #   torch
# tabulate==0.9.0
#     # via -r requirements.in
# tenacity==8.5.0
#     # via
#     #   langchain
#     #   langchain-community
#     #   langchain-core
#     #   llama-index-core
#     #   llama-index-legacy
#     #   streamlit
# threadpoolctl==3.5.0
#     # via scikit-learn
# tiktoken==0.6.0
#     # via
#     #   beyondllm
#     #   llama-index-core
#     #   llama-index-legacy
# tokenizers==0.19.1
#     # via
#     #   cohere
#     #   fastembed
#     #   transformers
# toml==0.10.2
#     # via streamlit
# torch==2.4.0
#     # via
#     #   llama-index-embeddings-adapter
#     #   sentence-transformers
# tornado==6.4.1
#     # via
#     #   ipykernel
#     #   jupyter-client
#     #   streamlit
# tqdm==4.66.5
#     # via
#     #   fastembed
#     #   google-generativeai
#     #   huggingface-hub
#     #   llama-index-core
#     #   nltk
#     #   openai
#     #   sentence-transformers
#     #   transformers
# traitlets==5.14.3
#     # via
#     #   comm
#     #   ipykernel
#     #   ipython
#     #   jupyter-client
#     #   jupyter-core
#     #   matplotlib-inline
# transformers==4.44.2
#     # via sentence-transformers
# types-requests==2.32.0.20240712
#     # via cohere
# typing-extensions==4.12.2
#     # via
#     #   altair
#     #   cohere
#     #   google-generativeai
#     #   huggingface-hub
#     #   ipython
#     #   langchain-core
#     #   llama-index-core
#     #   llama-index-legacy
#     #   openai
#     #   psycopg
#     #   psycopg-pool
#     #   pydantic
#     #   pydantic-core
#     #   sqlalchemy
#     #   streamlit
#     #   torch
#     #   typing-inspect
# typing-inspect==0.9.0
#     # via
#     #   dataclasses-json
#     #   llama-index-core
#     #   llama-index-legacy
# tzdata==2024.1
#     # via
#     #   pandas
#     #   psycopg
# urllib3==2.2.2
#     # via
#     #   botocore
#     #   gradientai
#     #   requests
#     #   types-requests
# watchdog==4.0.2
#     # via streamlit
# wcwidth==0.2.13
#     # via prompt-toolkit
# win32-setctime==1.1.0
#     # via loguru
# wrapt==1.16.0
#     # via
#     #   deprecated
#     #   langfuse
#     #   llama-index-core
# yarl==1.9.7
#     # via aiohttp
# youtube-transcript-api==0.6.2
#     # via llama-index-readers-youtube-transcript
